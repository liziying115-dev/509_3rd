{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704cbb05",
   "metadata": {},
   "source": [
    "# FizzBuzz\n",
    "Write a program that prints the numbers from 1 to 100. But for multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers that are multiples of both three and five print “FizzBuzz”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e4127",
   "metadata": {},
   "source": [
    "# Tic-Tac-Toe Game Winner\n",
    "Given a board of a tic-tac-toe game, find the winner. A sample board is given as follows:\n",
    "```py\n",
    "board = [['O', 'X', 'X'],\n",
    "         ['X', 'X', 'O'],\n",
    "         ['O', 'X', 'X']]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291bbfb",
   "metadata": {},
   "source": [
    "# Password Strenghth Meter\n",
    "Write a Python program that reads passwords from users. For the given password, tell users whether the password is **weak**, **okay**, or **strong**. Here are the scoring rules for a password:\n",
    "\n",
    "- Length\n",
    "  - +2 if length ≥ 12\n",
    "  - +1 if length is 8–11\n",
    "  - +0 otherwise\n",
    "- Character variety\n",
    "  - +1 if it has both lowercase [a-z] and uppercase [A-Z]\n",
    "  - +1 if it has at least one digit [0-9]\n",
    "  - +1 if it has at least one special from this set only: !@#$%^&*\n",
    "- Penalties\n",
    "  - −2 if it contains any whitespace (space, tab, etc.)\n",
    "  - −3 if it contains three or more identical characters in a row (e.g., aaa, 1111, ***)\n",
    "- Final Result\n",
    "  - weak if score < 3\n",
    "  - okay if 3 ≤ score ≤ 4\n",
    "  - strong if score ≥ 5\n",
    "\n",
    "Test your program with the following passwords:\n",
    "- P@ssw0rd\n",
    "- hunter2\n",
    "- CorrectHorseBatteryStaple\n",
    "- abcde Fgh\n",
    "- A!!!A!!!A!!!\n",
    "\n",
    "**Hint**: You may find `str.islower()`, `str.isupper()`, `str.isdigit()`, and `str.isspace()` useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15c7e4",
   "metadata": {},
   "source": [
    "# Vacuum Robot\n",
    "\n",
    "You get an $n\\times n$ map made of:\n",
    "- \\# = wall (can’t go through)\n",
    "- . = empty floor (can move)\n",
    "- S = where the robot starts (treated like empty floor)\n",
    "You’re told which way the robot is facing (N, E, S, or W).\n",
    "\n",
    "You get a string of commands:\n",
    "- L turn left 90°\n",
    "- R turn right 90°\n",
    "- F try to move one step forward\n",
    "If a forward step would hit a wall or leave the map, the robot remains where it is. Its health condition decreases by 1.\n",
    "After all commands, print: row col orientation (rows/cols are 1-based, top-left is (1,1)), as well as robot's health condition.\n",
    "**Note**: The minimum health condition is 0. Once the robot's health reaches 0, it cannot accept commands any more. \n",
    "\n",
    "Test your program with the following grids and commands:\n",
    "\n",
    "- Grid 1\n",
    "    ```py\n",
    "    #####\n",
    "    #...#\n",
    "    #.#S#\n",
    "    #...#\n",
    "    #####\n",
    "    ```\n",
    "    Commands: FFLFFRFFF; assume the robot initially facing east (E) with initial health 3.\n",
    "\n",
    "- Grid 2\n",
    "    ```py\n",
    "    #######\n",
    "    #..#..#\n",
    "    #.#.#S#\n",
    "    #.#...#\n",
    "    #..####\n",
    "    #.....#\n",
    "    #######\n",
    "    ```\n",
    "    Commands: FLLFFFRFFLFFRFF; assume the robot initially facing west (W) with initial health 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383cf976",
   "metadata": {},
   "source": [
    "# Optional Challenge: Toy Tokenizer for LLMs\n",
    "\n",
    "Large language models (LLMs) don’t read raw characters or whole words—they operate on integers. A tokenizer is the front end that turns text into a sequence of token IDs the model understands. In this assignment, you’ll implement a WordPiece-style greedy tokenizer.\n",
    "\n",
    "Write a program that tokenizes text lines into WordPiece-style subwords using the fixed vocabulary below. If a word cannot be fully segmented with the vocab, output `[UNK]` for that entire word.\n",
    "\n",
    "The tokenizer should be built based on the following rules: \n",
    "- Lowercase the given string.\n",
    "- Build words from letters a–z only. Any other character splits words.\n",
    "  - If the splitter is one of . , ! ? ( ) ; : → emit it as its own token.\n",
    "  - All other non-letters (digits, quotes, dashes, etc.) just split and are not emitted.\n",
    "- Inside each word, move left→right using greedy longest match:\n",
    "    - At start of word (i==0): you may use vocab pieces without the ## prefix.\n",
    "    - At i > 0 (continuation): you may use vocab pieces that do start with ##.\n",
    "    - Try the longest possible substring first; if not found, try shorter.\n",
    "    - If nothing matches at the current position, emit [UNK] for the whole word and stop processing that word.\n",
    "- Join tokens with single spaces. No leading/trailing spaces.\n",
    "\n",
    " \n",
    "Add 2–3 sentences in a markdown cell in your submission explaining what data type did you use to store vocabulary and why you chose it.\n",
    "\n",
    "Vocabulary (Root pieces without \\#):\n",
    "```cmd\n",
    "the\n",
    "a\n",
    "an\n",
    "i\n",
    "you\n",
    "we\n",
    "they\n",
    "it\n",
    "is\n",
    "are\n",
    "was\n",
    "were\n",
    "be\n",
    "have\n",
    "has\n",
    "do\n",
    "did\n",
    "not\n",
    "no\n",
    "go\n",
    "play\n",
    "read\n",
    "write\n",
    "un\n",
    "re\n",
    "in\n",
    "dis\n",
    "like\n",
    "love\n",
    "book\n",
    "game\n",
    "learn\n",
    "machine\n",
    "token\n",
    "data\n",
    "model\n",
    "```\n",
    "\n",
    "Vocabulary (Continuation with \\#):\n",
    "```cmd\n",
    "##s\n",
    "##es\n",
    "##ed\n",
    "##ing\n",
    "##er\n",
    "##ers\n",
    "##able\n",
    "##ment\n",
    "##tion\n",
    "##al\n",
    "##ly\n",
    "##ize\n",
    "##ized\n",
    "##ization\n",
    "##less\n",
    "##ful\n",
    "##read\n",
    "```\n",
    "Try your tokenizer with input \"Rewriting tokens → tokenization is fun.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
